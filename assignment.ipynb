{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fcera10/Lab04/blob/main/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "Here import all crucial packages etc."
      ],
      "metadata": {
        "collapsed": false,
        "id": "Ywu7bCi4kqU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "outputs": [],
      "source": [
        "# Code here\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import torch\n",
        "from transformers import EvalPrediction, pipeline"
      ],
      "metadata": {
        "id": "SHTkT10_kqU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "10i0X8czkqU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils\n",
        "\n",
        "Helper functions that you will use"
      ],
      "metadata": {
        "collapsed": false,
        "id": "IanbLSmjkqU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "outputs": [],
      "source": [
        "#Code here\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "0l6eOMqskqU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "outputs": [],
      "source": [
        "class DisinformationDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    This class wraps our tokenized data and labels so PyTorch can easily loop through them during training. It converts each input into tensors and returns them with the label — all in the format the model expects.\n",
        "    \"\"\"\n",
        "    # When we create an instance of dataset, we pass in encodings and labels\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    # This method tells PyTorch how to get one item (input + label).\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    # Returns how many examples are in the dataset (needed by DataLoader).\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "def load_and_process_data(file_path: str, label_column: str = \"label\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads the data from a CSV file and processes the labels.\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file.\n",
        "        label_column (str): The column name containing the labels.\n",
        "        text_column (str): The column name containing the text content.\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed dataframe with labels and text content.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(file_path, encoding='utf-8')\n",
        "    data[label_column] = data[label_column].apply(lambda x: 1 if \"fake\" in x.lower() else 0)\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_metrics_to_json(metrics: dict, output_file_path: str):\n",
        "    \"\"\"\n",
        "    Saves the metrics to a JSON file.\n",
        "    Args:\n",
        "        metrics (dict): The evaluation metrics.\n",
        "        output_file_path (str): The file path to save the metrics.\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        json.dump(metrics, output_file, indent=4)\n",
        "\n",
        "def compute_metrics(pred=None, y_true=None, y_pred=None):\n",
        "    \"\"\"\n",
        "    Computes F1 scores (micro, macro, weighted) for both training and testing data.\n",
        "\n",
        "    If `pred` is provided, it computes metrics for the trainer using `EvalPrediction`.\n",
        "    If `y_true` and `y_pred` are provided, it computes metrics for test data predictions.\n",
        "\n",
        "    Parameters:\n",
        "        - pred (EvalPrediction, optional): The evaluation prediction object for Trainer.\n",
        "        - y_true (list, optional): The ground truth labels for the test data.\n",
        "        - y_pred (list, optional): The predicted labels for the test data.\n",
        "\n",
        "    Returns:\n",
        "        - dict: A dictionary containing F1 metrics.\n",
        "    \"\"\"\n",
        "    if pred is not None:\n",
        "        # When working with the Trainer, pred is an EvalPrediction object\n",
        "        labels = pred.label_ids\n",
        "        y_pred = pred.predictions.argmax(-1)\n",
        "    elif y_true is not None and y_pred is not None:\n",
        "        # If y_true and y_pred are provided, use them for test evaluation\n",
        "        labels = y_true\n",
        "    else:\n",
        "        raise ValueError(\"Either `pred` or both `y_true` and `y_pred` must be provided.\")\n",
        "\n",
        "        # Compute F1 scores\n",
        "    f1 = f1_score(y_true=labels, y_pred=y_pred)\n",
        "\n",
        "    return {\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "def compute_metrics_for_trainer(pred: EvalPrediction):\n",
        "    return compute_metrics(pred=pred)"
      ],
      "metadata": {
        "id": "TztmRfDPkqU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "# Fine-Tuning BERT Model to Fake News detection"
      ],
      "metadata": {
        "collapsed": false,
        "id": "lQfnBpkrkqU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Train, Validation and Test data\n",
        "\n",
        "Import all datasets and load and preprocess train and validation\n",
        "\n",
        "Link to direcotry with data: https://github.com/ArkadiusDS/NLP-Labs/tree/master/data/CoAID/"
      ],
      "metadata": {
        "collapsed": false,
        "id": "n-vITRuWkqU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-19 10:21:21--  https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/CoAID/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 221757 (217K) [text/plain]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "\rtest.csv              0%[                    ]       0  --.-KB/s               \rtest.csv            100%[===================>] 216.56K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-05-19 10:21:21 (10.4 MB/s) - ‘test.csv’ saved [221757/221757]\n",
            "\n",
            "--2025-05-19 10:21:21--  https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/CoAID/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1556530 (1.5M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   1.48M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-05-19 10:21:21 (32.2 MB/s) - ‘train.csv’ saved [1556530/1556530]\n",
            "\n",
            "--2025-05-19 10:21:21--  https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/CoAID/validation.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 440110 (430K) [text/plain]\n",
            "Saving to: ‘validation.csv’\n",
            "\n",
            "validation.csv      100%[===================>] 429.79K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-19 10:21:22 (13.2 MB/s) - ‘validation.csv’ saved [440110/440110]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the URLs pointing to the raw CSV data files hosted on GitHub.\n",
        "\n",
        "url_test = 'https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/CoAID/test.csv'\n",
        "url_train = 'https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/CoAID/train.csv'\n",
        "url_valid = 'https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/CoAID/validation.csv'\n",
        "\n",
        "# Download the datasets from GitHub using the wget command-line tool.\n",
        "# Each file is saved with a simple filename for ease of use.\n",
        "\n",
        "!wget -O test.csv {url_test}\n",
        "!wget -O train.csv {url_train}\n",
        "!wget -O validation.csv {url_valid}\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T20:13:41.486063Z",
          "start_time": "2025-05-14T20:13:41.483303Z"
        },
        "id": "jueS4Ro-kqU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de36f01-fa57-4a5b-cd40-28f12e970f12"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   uuid  \\\n",
              "0  04355d1f-02e4-4823-8e7d-9fce5db7c884   \n",
              "1  1c1dbbcc-e0e2-4da6-b3b5-60489fa04024   \n",
              "2  cf880aed-6724-4e19-b601-1417bf9bf715   \n",
              "3  7a4e0ab3-e150-409d-8e96-fac591cb07d1   \n",
              "4  4af86d9d-4951-4124-bf30-7356bbcae1b4   \n",
              "\n",
              "                                             content  label  article_type  \\\n",
              "0  \"Contrary to claims in viral social media post...      1  twitter post   \n",
              "1                \"What is herd immunity? | @scoopit\"      0  twitter post   \n",
              "2  march 23 2020 -- the fda has approved a rapid ...      0  news article   \n",
              "3  people may be disinfecting their food to avoid...      0  news article   \n",
              "4  \"Target calls (those affected by gambling) ans...      0  twitter post   \n",
              "\n",
              "  source_type pre_post_GPT dataset_source  text_length  \n",
              "0       human      pre-GPT          CoAID           30  \n",
              "1       human      pre-GPT          CoAID           10  \n",
              "2       human      pre-GPT          CoAID           94  \n",
              "3       human      pre-GPT          CoAID           80  \n",
              "4       human      pre-GPT          CoAID           43  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a223437c-b370-4232-9cc3-12bfdd50bae3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "      <th>article_type</th>\n",
              "      <th>source_type</th>\n",
              "      <th>pre_post_GPT</th>\n",
              "      <th>dataset_source</th>\n",
              "      <th>text_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>04355d1f-02e4-4823-8e7d-9fce5db7c884</td>\n",
              "      <td>\"Contrary to claims in viral social media post...</td>\n",
              "      <td>1</td>\n",
              "      <td>twitter post</td>\n",
              "      <td>human</td>\n",
              "      <td>pre-GPT</td>\n",
              "      <td>CoAID</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1c1dbbcc-e0e2-4da6-b3b5-60489fa04024</td>\n",
              "      <td>\"What is herd immunity? | @scoopit\"</td>\n",
              "      <td>0</td>\n",
              "      <td>twitter post</td>\n",
              "      <td>human</td>\n",
              "      <td>pre-GPT</td>\n",
              "      <td>CoAID</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cf880aed-6724-4e19-b601-1417bf9bf715</td>\n",
              "      <td>march 23 2020 -- the fda has approved a rapid ...</td>\n",
              "      <td>0</td>\n",
              "      <td>news article</td>\n",
              "      <td>human</td>\n",
              "      <td>pre-GPT</td>\n",
              "      <td>CoAID</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7a4e0ab3-e150-409d-8e96-fac591cb07d1</td>\n",
              "      <td>people may be disinfecting their food to avoid...</td>\n",
              "      <td>0</td>\n",
              "      <td>news article</td>\n",
              "      <td>human</td>\n",
              "      <td>pre-GPT</td>\n",
              "      <td>CoAID</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4af86d9d-4951-4124-bf30-7356bbcae1b4</td>\n",
              "      <td>\"Target calls (those affected by gambling) ans...</td>\n",
              "      <td>0</td>\n",
              "      <td>twitter post</td>\n",
              "      <td>human</td>\n",
              "      <td>pre-GPT</td>\n",
              "      <td>CoAID</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a223437c-b370-4232-9cc3-12bfdd50bae3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a223437c-b370-4232-9cc3-12bfdd50bae3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a223437c-b370-4232-9cc3-12bfdd50bae3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7f7e795e-c2bf-4e07-84b6-16c5c7097890\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7f7e795e-c2bf-4e07-84b6-16c5c7097890')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7f7e795e-c2bf-4e07-84b6-16c5c7097890 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_data",
              "summary": "{\n  \"name\": \"train_data\",\n  \"rows\": 3507,\n  \"fields\": [\n    {\n      \"column\": \"uuid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3507,\n        \"samples\": [\n          \"8dac3676-f278-4709-904f-c7fe00ba6c17\",\n          \"06fc1b64-884b-456f-835e-0608a1f2da7c\",\n          \"172926dd-9e19-471e-91a0-d2c1ad56a975\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3507,\n        \"samples\": [\n          \"an analysis of more than 15,000 sars cov-2 genomes finds that mutations to the virus do not increase its transmissibility and are instead either neutral or detrimental to its spread. new research examines the mutations to the new coronavirus. all data and statistics are based on publicly available data at the time of publication. some information may be out of date. visit our coronavirus hub and follow our live updates page for the most recent information on the covid-19 outbreak. since the emer. \",\n          \"the president okay thank you very much. today is world elder abuse awareness day and were here to discuss our ironclad commitment to protecting and caring for americas seniors. were joined by vice president mike pence attorney general william barr secretary of health and human services alex azar secretary of housing and urban development ben carson who by the way was fantastic over the weekend in various interviews you did ben. really good job i appreciate it. secretary of vetera. \",\n          \"\\\"Can people wear masks while exercising during COVID-19? People should NOT wear masks when exercising as masks may reduce the ability to breathe comfortably. #masks #nomasksonexercising #corona #covi19 #arcadiaprojects\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"news article\",\n          \"twitter post\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"human\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pre_post_GPT\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"pre-GPT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dataset_source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"CoAID\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24,\n        \"min\": 7,\n        \"max\": 106,\n        \"num_unique_values\": 98,\n        \"samples\": [\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# Load and preprocess the datasets using the custom function 'load_and_process_data'\n",
        "# This function will load the CSV data files, process the labels, and return the data in a usable dataframe format.\n",
        "\n",
        "# Load and process the training data\n",
        "train_data = load_and_process_data('train.csv')\n",
        "\n",
        "# Load and process the validation data\n",
        "validation_data = load_and_process_data('validation.csv')\n",
        "\n",
        "train_data.head()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T20:13:42.046761Z",
          "start_time": "2025-05-14T20:13:42.040214Z"
        },
        "id": "fxzRA12gkqU3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "3f9c0d55-5a13-497c-abcf-6cdf5c0f34e5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and tokenizer\n",
        "\n",
        "Firstly create two dicts id2label and label2id and then load model and tokenizer\n",
        "Then use well-known distilled version of BERT model for faster fine-tuning: 'distilbert/distilbert-base-uncased' or any other model you wish."
      ],
      "metadata": {
        "collapsed": false,
        "id": "N8EsDwHCkqU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "id2label = {0: \"Credible\", 1: \"Fake\"}\n",
        "label2id = {\"Credible\": 0, \"Fake\": 1}\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "# BERT is a transformer-based model that has been pre-trained on a large corpus of text\n",
        "# We'll use it for classification task, where the model predicts labels for text.\n",
        "\n",
        "# Load the BERT model for classification (the base uncased version of BERT)\n",
        "# This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created with the from_pretrained()\n",
        "model = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-uncased', num_labels=2, id2label=id2label, label2id=label2id)\n",
        "\n",
        "# Load the corresponding tokenizer for BERT\n",
        "# The tokenizer is responsible for converting the text into tokens that the model can process\n",
        "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
        "\n",
        "\n",
        "# Tokenize the datasets (training and validation) to prepare them for input into the BERT model.\n",
        "# Tokenization converts the raw text data into a format the BERT model can process.\n",
        "\n",
        "# Tokenizing the training dataset\n",
        "train_encodings = tokenizer(\n",
        "        train_data['content'].tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "# Tokenizing the validation dataset\n",
        "val_encodings = tokenizer(\n",
        "        validation_data['content'].tolist(),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=256\n",
        "    )"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T20:13:45.278032Z",
          "start_time": "2025-05-14T20:13:45.273486Z"
        },
        "id": "zI1PEZQYkqU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959f4544-a1b6-4b68-ce8e-fd10edaf346a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize datasets and prepare it for fine-tuning\n",
        "\n",
        "You may use DisinformationDataset class for data preparation."
      ],
      "metadata": {
        "collapsed": false,
        "id": "whiddemzkqU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "outputs": [],
      "source": [
        "# Create custom datasets for training and validation using the DisinformationDataset class.\n",
        "# These datasets will format the tokenized text data and corresponding labels into a format that can be used by the model during training and evaluation.\n",
        "\n",
        "# Create the training dataset: it combines the tokenized training data and corresponding labels\n",
        "train_dataset = DisinformationDataset(train_encodings, train_data['label'].tolist())\n",
        "\n",
        "# Create the validation dataset: it combines the tokenized validation data and corresponding labels\n",
        "val_dataset = DisinformationDataset(val_encodings, validation_data['label'].tolist())"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T20:13:47.959638Z",
          "start_time": "2025-05-14T20:13:47.955082Z"
        },
        "id": "4GLUlMewkqU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune BERT model on at least 3 sets of hyperparameters\n",
        "\n",
        "Check F1 score, precision and recall for each fine-tuned model and at the end choose set of hyperparameters that gives you best results. For each set of hyperparameters write down the final metrics. You need to acheive at least below result on validation dataset:\n",
        "\n",
        "\"f1\": 0.91,\n",
        "\"recall\": 0.91,\n",
        "\"precision\": 0.91\n",
        "\n",
        "Remember you need to achieve these minimum results on VALIDATION dataset and the best model on validation dataset will have to be used for predictions on test dataset.\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "fNkGfmvckqU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Save the trained model to a specified directory after training is completed.\\n# This allows you to persist the model and use it for future predictions or fine-tuning without retraining.\\nmodel_saved_path='output/final/'\\ntrainer.save_model(model_saved_path)\\ntokenizer.save_pretrained(model_saved_path)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "# https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments\n",
        "'''training_args = TrainingArguments(\n",
        "    output_dir='output/training/',\n",
        "    eval_strategy='steps',\n",
        "    learning_rate=0.00001,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    warmup_ratio=0.06,\n",
        "    weight_decay=0.1,\n",
        "    fp16=True,\n",
        "    metric_for_best_model='f1',\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    greater_is_better=True,\n",
        "    save_strategy='steps',\n",
        "    eval_steps=100,\n",
        "    save_on_each_node=True,\n",
        "    report_to=[]\n",
        ")'''\n",
        "\n",
        "'''\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,  # Pass the actual model instance\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics_for_trainer\n",
        "    )\n",
        "# Train the model using the Trainer class.\n",
        "# This method will start the training process based on the configurations specified in the TrainingArguments.\n",
        "# The model will learn from the training data and be evaluated on the validation data according to the provided settings.\n",
        "\n",
        "trainer.train()'''\n",
        "'''\n",
        "# Save the trained model to a specified directory after training is completed.\n",
        "# This allows you to persist the model and use it for future predictions or fine-tuning without retraining.\n",
        "model_saved_path='output/final/'\n",
        "trainer.save_model(model_saved_path)\n",
        "tokenizer.save_pretrained(model_saved_path)'''"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T20:13:50.601803Z",
          "start_time": "2025-05-14T20:13:50.598773Z"
        },
        "id": "lqYve2i7kqU3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1ea9d740-2ec7-44df-b325-7434a7568bae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments\n",
        "#compare matrix\n",
        "def compute_metrics(pred=None,y_true=None,y_pred=None,average:str='binary'):\n",
        "  if pred is not None:\n",
        "    lab=pred.label_ids\n",
        "    y_pred=pred.predictions.argmax(-1)\n",
        "  elif y_true is not None and y_pred is not None:\n",
        "    lab=y_true\n",
        "  else:\n",
        "    raise ValueError(\"Need pred or both y_true and y_pred\")\n",
        "  prec=precision_score(y_true=lab,y_pred=y_pred,average=average)\n",
        "  rec=recall_score(y_true=lab,y_pred=y_pred,average=average)\n",
        "  f1=f1_score(y_true=lab,y_pred=y_pred,average=average)\n",
        "  return {\n",
        "\n",
        "      'precision':prec,\n",
        "      'recall':rec,\n",
        "      'f1':f1\n",
        "  }\n",
        "\n",
        "# Esperimento 0: baseline\n",
        "training_args_0 = TrainingArguments(\n",
        "    output_dir='output/exp_0/',\n",
        "    eval_strategy='steps',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    warmup_ratio=0.06,\n",
        "    weight_decay=0.1,\n",
        "    fp16=True,\n",
        "    metric_for_best_model='f1',\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    greater_is_better=True,\n",
        "    save_strategy='steps',\n",
        "    eval_steps=100,\n",
        "    save_on_each_node=True,\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "# Esperimento 1: learning rate più alto\n",
        "training_args_1 = TrainingArguments(\n",
        "    output_dir='output/exp_1/',\n",
        "    eval_strategy='steps',\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    warmup_ratio=0.06,\n",
        "    weight_decay=0.1,\n",
        "    fp16=True,\n",
        "    metric_for_best_model='f1',\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    greater_is_better=True,\n",
        "    save_strategy='steps',\n",
        "    eval_steps=100,\n",
        "    save_on_each_node=True,\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "# Esperimento 2: batch size maggiore\n",
        "training_args_2 = TrainingArguments(\n",
        "    output_dir='output/exp_2/',\n",
        "    eval_strategy='steps',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=5,\n",
        "    warmup_ratio=0.06,\n",
        "    weight_decay=0.1,\n",
        "    fp16=True,\n",
        "    metric_for_best_model='f1',\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    greater_is_better=True,\n",
        "    save_strategy='steps',\n",
        "    eval_steps=100,\n",
        "    save_on_each_node=True,\n",
        "    report_to=[]\n",
        ")\n"
      ],
      "metadata": {
        "id": "JyO24vdFx1UK"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compare matrix\n",
        "def compute_metrics(pred=None,y_true=None,y_pred=None,average:str='binary'):\n",
        "  if pred is not None:\n",
        "    lab=pred.label_ids\n",
        "    y_pred=pred.predictions.argmax(-1)\n",
        "  elif y_true is not None and y_pred is not None:\n",
        "    lab=y_true\n",
        "  else:\n",
        "    raise ValueError(\"Need pred or both y_true and y_pred\")\n",
        "  prec=precision_score(y_true=lab,y_pred=y_pred,average=average)\n",
        "  rec=recall_score(y_true=lab,y_pred=y_pred,average=average)\n",
        "  f1=f1_score(y_true=lab,y_pred=y_pred,average=average)\n",
        "  return {\n",
        "\n",
        "      'precision':prec,\n",
        "      'recall':rec,\n",
        "      'f1':f1\n",
        "  }"
      ],
      "metadata": {
        "id": "WZlDaajuyHCi"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the code\n",
        "'''trainer = Trainer(\n",
        "        model=model,  # Pass the actual model instance\n",
        "        args=training_args_0,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )'''\n",
        "# Train the model using the Trainer class.\n",
        "# This method will start the training process based on the configurations specified in the TrainingArguments.\n",
        "# The model will learn from the training data and be evaluated on the validation data according to the provided settings.\n",
        "\n",
        "#trainer.train()\n",
        "# Save the trained model to a specified directory after training is completed.\n",
        "# This allows you to persist the model and use it for future predictions or fine-tuning without retraining.\n",
        "'''model_saved_path='output/final/'\n",
        "trainer.save_model(model_saved_path)\n",
        "tokenizer.save_pretrained(model_saved_path)'''\n",
        "\n",
        "def train_fn(model, train_arg,train_dataset, val_dataset, compute_metrics):\n",
        "  Trainer(\n",
        "        model=model,  # Pass the actual model instance\n",
        "        args=train_arg,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "  )\n",
        "  trainer.train()\n",
        "  return trainer.evaluate()\n",
        "eval_0=train_fn(model, training_args_0 ,train_dataset, val_dataset, compute_metrics)\n",
        "eval_1=train_fn(model, training_args_1 ,train_dataset, val_dataset, compute_metrics)\n",
        "eval_2=train_fn(model, training_args_2 ,train_dataset, val_dataset, compute_metrics)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PrOHKDBGyBXz",
        "outputId": "4feeccbe-d036-440c-f763-bff5898962e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1100/1100 06:11, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.239591</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.239734</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.242072</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.244336</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245037</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245683</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245266</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.246014</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.248546</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.248627</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.248953</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 08:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:26]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1100/1100 06:26, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.253153</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.253495</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.255650</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.257999</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.258754</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.259449</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.258983</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.259522</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.261864</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.261930</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.262252</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:26]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1100/1100 05:39, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.266170</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.266614</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.268752</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.270928</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.271617</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272260</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.271776</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272278</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.274559</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.274547</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.274855</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l_0=[eval_0['eval_precision'],eval_0['eval_recall'],eval_0['eval_f1']]\n",
        "m_0=np.mean(l_0)\n",
        "l_1=[eval_1['eval_precision'],eval_1['eval_recall'],eval_1['eval_f1']]\n",
        "m_1=np.mean(l_1)\n",
        "l_2=[eval_2['eval_precision'],eval_2['eval_recall'],eval_2['eval_f1']]\n",
        "m_2=np.mean(l_2)\n",
        "\n",
        "if l_0>l_1:\n",
        "  if l_0>l_2:\n",
        "    best_train_arg=training_args_0\n",
        "    print('The best is arg 0')\n",
        "  else:\n",
        "    best_train_arg=training_args_2\n",
        "elif l_1>l_2:\n",
        "  best_train_arg=training_args_1\n",
        "  print('The best is arg 1')\n",
        "else:\n",
        "  best_train_arg=training_args_2\n",
        "  print('The best is arg 3')\n",
        "\n",
        "Trainer(\n",
        "        model=model,  # Pass the actual model instance\n",
        "        args=best_train_arg,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "  )\n",
        "trainer.train()\n",
        "\n",
        "model_saved_path='output/final/'\n",
        "trainer.save_model(model_saved_path)\n",
        "tokenizer.save_pretrained(model_saved_path)"
      ],
      "metadata": {
        "id": "3zMmTR0p93F6",
        "outputId": "64618b6d-90a0-4aeb-b5e9-9cfc7a86a723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1101' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1100/1100 04:43, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.278412</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.278742</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.280636</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.282609</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.283104</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.283751</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.283275</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.283649</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.285863</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.285906</td>\n",
              "      <td>0.994595</td>\n",
              "      <td>0.897561</td>\n",
              "      <td>0.943590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/63 00:01 < 00:01, 24.52 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 20:24]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final prediction on test dataset\n",
        "\n",
        "Take best model and hyperparameters on validation and predict on test dataset. Compute evaluation metrics f1, precision and recall."
      ],
      "metadata": {
        "collapsed": false,
        "id": "QrKLQ-EykqU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "'''# Load the test data and preprocess\n",
        "test_data = load_and_process_data('test.csv')\n",
        "\n",
        "# Load the pipeline with CUDA\n",
        "classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=model_saved_path,\n",
        "    tokenizer=model_saved_path,\n",
        "    device=0,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "# Run pipeline on all content (batched)\n",
        "results = classifier(test_data[\"content\"].tolist(), batch_size=32)\n",
        "\n",
        "# Convert results to binary predictions\n",
        "test_data[\"predictions\"] = [1 if r[\"label\"] == \"Fake\" else 0 for r in results]\n",
        "\n",
        "\n",
        "# Compute evaluation metrics on the test data\n",
        "evaluation_results = compute_metrics(y_true=test_data[\"label\"], y_pred=test_data[\"predictions\"])\n",
        "\n",
        "# Save the evaluation metrics to a JSON file\n",
        "output_file_path = \"metrics/results.json\"\n",
        "save_metrics_to_json(evaluation_results, output_file_path)'''"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T19:44:16.981717Z",
          "start_time": "2025-05-14T19:44:16.918294Z"
        },
        "id": "aPSiPRe8kqU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"experiment_0\": {\n",
        "        \"model\": \"bert-base-uncased\",\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": 2e-5,\n",
        "            \"per_device_train_batch_size\": 16\n",
        "        },\n",
        "        \"f1_score\": eval_0[\"eval_f1\"],\n",
        "        \"precision\": eval_0[\"eval_precision\"],\n",
        "        \"recall\": eval_0[\"eval_recall\"],\n",
        "        \"description\": \"Baseline con learning rate 2e-5 e batch size 16.\"\n",
        "    },\n",
        "    \"experiment_1\": {\n",
        "        \"model\": \"bert-base-uncased\",\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": 3e-5,\n",
        "            \"per_device_train_batch_size\": 16\n",
        "        },\n",
        "        \"f1_score\": eval_1[\"eval_f1\"],\n",
        "        \"precision\": eval_1[\"eval_precision\"],\n",
        "        \"recall\": eval_1[\"eval_recall\"],\n",
        "        \"description\": \"Learning rate aumentato a 3e-5 per maggiore velocità di apprendimento.\"\n",
        "    },\n",
        "    \"experiment_2\": {\n",
        "        \"model\": \"bert-base-uncased\",\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": 2e-5,\n",
        "            \"per_device_train_batch_size\": 32\n",
        "        },\n",
        "        \"f1_score\": eval_2[\"eval_f1\"],\n",
        "        \"precision\": eval_2[\"eval_precision\"],\n",
        "        \"recall\": eval_2[\"eval_recall\"],\n",
        "        \"description\": \"Batch size maggiore per stabilizzare l'apprendimento.\"\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "5wv6xdL5yXLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final file with results and description"
      ],
      "metadata": {
        "collapsed": false,
        "id": "KCpihP8QkqU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-04T15:21:11.331626Z",
          "start_time": "2025-05-04T15:21:11.312816Z"
        },
        "id": "JyyfFl2CkqU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All keys in your dictionary have to be the same as below. The only changes you should do in terms of keys is changing names of hyperparameters, e.g. instead of key \"name_of_hyperparameter_0\" if you used learning rate then write \"learning_rate\". Other important information in the dictionary below and comments. Each value says what is expected.\n",
        "\n",
        "Example dictionary provided under the template."
      ],
      "metadata": {
        "collapsed": false,
        "id": "XuVwdSYtkqU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template for your structured resulting file"
      ],
      "metadata": {
        "collapsed": false,
        "id": "EEqedKdUkqU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "data = {\n",
        "    # Everything in experiment_0 is related to experiment on validation dataset, so metrics are computed on validation dataset etc.\n",
        "    \"experiment_0\": {\n",
        "        \"model\": \"model name\",\n",
        "        \"hyperparameters\": {\n",
        "            \"name_of_hyperparameter_0\": \"value in str or float - You need to play with at least two different hyperparameters so at least name_of_hyperparameter_0 and name_of_hyperparameter_1\",\n",
        "            \"name_of_hyperparameter_1\": \"value in str or float\"\n",
        "        },\n",
        "        \"f1_score\": \"value in float\",\n",
        "        \"precision\": \"value in float\",\n",
        "        \"recall\": \"value in float\",\n",
        "        \"description\": \"Unique description one of the approach - it has to be different for each experiment.\"\n",
        "    },\n",
        "    # Everything in experiment_1 is related to experiment on validation dataset, so metrics are computed on validation dataset etc.\n",
        "    \"experiment_1\": {\n",
        "        \"model\": \"model name\",\n",
        "        \"hyperparameters\": {\n",
        "            \"name_of_hyperparameter_0\": \"value in str or float\",\n",
        "            \"name_of_hyperparameter_1\": \"value in str or float\"\n",
        "        },\n",
        "        \"f1_score\": \"value in float\",\n",
        "        \"precision\": \"value in float\",\n",
        "        \"recall\": \"value in float\",\n",
        "        \"description\": \"Unique description two of the approach - it has to be different for each experiment.\"\n",
        "    },\n",
        "    # Everything in experiment_2 is related to experiment on validation dataset, so metrics are computed on validation dataset etc.\n",
        "    \"experiment_2\": {\n",
        "        \"model\": \"model name\",\n",
        "        \"hyperparameters\": {\n",
        "            \"name_of_hyperparameter_0\": \"value in str or float\",\n",
        "            \"name_of_hyperparameter_1\": \"value in str or float\"\n",
        "        },\n",
        "        \"f1_score\": \"value in float\",\n",
        "        \"precision\": \"value in float\",\n",
        "        \"recall\": \"value in float\",\n",
        "        \"description\": \"Unique description three of the approach - it has to be different for each experiment.\"\n",
        "    },\n",
        "    # Everything in final_prediction is related to prediction on test dataset, so metrics are computed on test dataset etc.\n",
        "    \"final_prediction\": {\n",
        "        \"model\": \"google-bert/bert-base-uncased\",\n",
        "        \"experiment_chosen\": \"experiment_0 or experiment_1 or experiment_2\",\n",
        "        \"hyperparameters\": {\n",
        "            \"name_of_hyperparameter_0\": \"value in str or float\",\n",
        "            \"name_of_hyperparameter_1\": \"value in str or float\"\n",
        "        },\n",
        "        \"f1_score\": \"value in float\",\n",
        "        \"precision\": \"value in float\",\n",
        "        \"recall\": \"value in float\",\n",
        "        \"description\": \"Unique description four of the final results and prediction - it has to be different and here you will describe results on test dataset.\"\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T13:15:27.971126Z",
          "start_time": "2025-05-14T13:15:27.959036Z"
        },
        "id": "_QepKIF8kqU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "with open(\"experiments_name_surname_student_id.json\", \"w\") as f:\n",
        "    json.dump(data, f, indent=4)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T13:15:36.578291Z",
          "start_time": "2025-05-14T13:15:36.564453Z"
        },
        "id": "6Lidmk2-kqU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example final file"
      ],
      "metadata": {
        "collapsed": false,
        "id": "9hEchgw9kqU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"experiment_0\": {\n",
        "        \"model\": \"google-bert/bert-base-uncased\",\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": \"float\",\n",
        "            \"warmap_ratio\": \"float\",\n",
        "            \"weight_decay\": \"float\"\n",
        "        },\n",
        "        \"f1_score\": \"float\",\n",
        "        \"precision\": \"float\",\n",
        "        \"recall\": \"float\",\n",
        "        \"description\": \"This experiment fine-tuned the google-bert/bert-base-uncased model for binary classification using a learning rate of 1e-5 and a warmup ratio of 0.06. The model achieved an F1-score of 0.76, with a strong recall of 0.85, indicating high sensitivity to positive cases. Precision was moderate at 0.65, suggesting some trade-off in false positives. The setup demonstrates effective recall-oriented performance in identifying relevant instances.\"\n",
        "    },\n",
        "    \"experiment_1\": {\n",
        "        \"model\": \"google-bert/bert-base-uncased\",\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": \"float\",\n",
        "            \"weight_decay\": \"float\"\n",
        "        },\n",
        "        \"f1_score\": \"float\",\n",
        "        \"precision\": \"float\",\n",
        "        \"recall\": \"float\",\n",
        "        \"description\": \"Unique description two of the approach - it has to be different for each experiment. Everything in experiment_1 is related to experiment on validation dataset, so metrics are computed on validation dataset etc.\"\n",
        "    },\n",
        "    \"experiment_2\": {\n",
        "        \"model\": \"google-bert/bert-base-uncased\",\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": \"float\",\n",
        "            \"num_train_epochs\": \"int\",\n",
        "            \"weight_decay\": \"float\"\n",
        "        },\n",
        "        \"f1_score\": \"float\",\n",
        "        \"precision\": \"float\",\n",
        "        \"recall\": \"float\",\n",
        "        \"description\": \"Unique description three of the approach - it has to be different for each experiment. Everything in experiment_2 is related to experiment on validation dataset, so metrics are computed on validation dataset etc.\"\n",
        "    },\n",
        "    \"final_prediction\": {\n",
        "        \"model\": \"google-bert/bert-base-uncased\",\n",
        "        \"experiment_chosen\": \"experiment_0\",\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": \"float\",\n",
        "            \"warmap_ratio\": \"float\"\n",
        "        },\n",
        "        \"f1_score\": \"float\",\n",
        "        \"precision\": \"float\",\n",
        "        \"recall\": \"float\",\n",
        "        \"description\": \"Unique description four of the final results and prediction - it has to be different and here you will describe results on test dataset. Everything in final_prediction is related to prediction on test dataset, so metrics are computed on test dataset etc.\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T13:16:07.131068Z",
          "start_time": "2025-05-14T13:16:07.123378Z"
        },
        "id": "lAg2J8fekqU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "with open(\"experiments_Arkadiusz_Modzelewski_29580.json\", \"w\") as f:\n",
        "    json.dump(data, f, indent=4)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-14T13:16:08.560046Z",
          "start_time": "2025-05-14T13:16:08.553870Z"
        },
        "id": "OP17hybBkqU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "3lCRGoNmkqU4"
      }
    }
  ]
}